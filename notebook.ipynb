{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=openai_api_key, temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items)) \n",
    "        \n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 리스트를 생성합니다. 값은 콤마로만 구분해주세요.\"),\n",
    "    (\"system\", \"리스트 크기는 {max_length}를 넘을 수 없고, 값은 중복될 수 없습니다.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# prompt = template.format_messages(max_length=10, question=\"국가들을 알려주세요.\" )\n",
    "# result = chat.predict_messages(prompt)\n",
    "# p = CommaOutputParser()\n",
    "# p.parse(result.content)\n",
    "\n",
    "chain = template | chat | CommaOutputParser()\n",
    "chain.invoke({\n",
    "    \"max_length\":5,\n",
    "    \"question\":\"태국의 도시들을 알려주세요.\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=openai_api_key, temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "\n",
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 월드클래스 요리사입니다. 당신은 구하기 쉬운 재료로 쉽게 따라할 수 있는 요리 레시피를 만들 수 있습니다.\"),\n",
    "    (\"human\", \"저는 {country} 요리를 만들고 싶습니다.\")\n",
    "])\n",
    "chef_chain = chef_prompt | chat\n",
    "\n",
    "\n",
    "veg_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 월드클래스 채식 요리사입니다. 당신은 전달 받은 레시피를 통해 채식 요리 레시피로 변경할 수 있습니다.\"),\n",
    "    (\"system\", \"대체 재료를 명확히 찾을 수 없거나, 레시피를 너무 많이 바꿔야 한다면 할 수 없다고 말해야 합니다.\"),\n",
    "    (\"human\", \"{recipe}\")\n",
    "])\n",
    "\n",
    "veg_chain = veg_prompt | chat\n",
    "final_chain = {\"recipe\" : chef_chain} | veg_chain\n",
    "result = final_chain.invoke({\"country\":\"한국\"}).content\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PipelinePromptTemplate, PromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    openai_api_key = openai_api_key, \n",
    "    temperature = 0.1, \n",
    "    streaming = True, \n",
    "    callbacks = [\n",
    "        StreamingStdOutCallbackHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are a role playing assistant.\n",
    "And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "This is an example of how you talk:\n",
    "\n",
    "Human: {example_question}\n",
    "You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Start now!\n",
    "\n",
    "Human: {question}\n",
    "You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "{intro}\n",
    "\n",
    "{example}\n",
    "\n",
    "{start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start)\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final, \n",
    "    pipeline_prompts=prompts\n",
    ")\n",
    "\n",
    "\n",
    "chain = full_prompt | chat;\n",
    "\n",
    "chain.invoke({\n",
    "    \"character\": \"Dragon Slayer\",\n",
    "    \"example_question\": \"What is your location?\",\n",
    "    \"example_answer\": \"That is a secret!\",\n",
    "    \"question\": \"What is you fav food?\",\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
